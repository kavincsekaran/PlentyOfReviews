{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('C:/JK/Masters Studies/Spring 2018/Information Retrieval/Project/data/reviews.csv/reviews.csv')\n",
    "# reviews['listing_id'].value_counts()\n",
    "# vc = reviews['listing_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_less = reviews.groupby('listing_id').filter(lambda g: (g.listing_id.size >= 5) and (g.listing_id.size < 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_nonalphanumeric(text):\n",
    "    return re.sub(\"[^a-zA-Z0-9]\",\" \", str(text)).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3986"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_listings = reviews.listing_id.unique()\n",
    "len(unique_listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews['comments_1'] = reviews['comments'].apply(remove_nonalphanumeric)\n",
    "reviews['tokenized_comments'] = reviews['comments_1'].apply(word_tokenize)\n",
    "stopset = stopwords.words('english') + list(string.punctuation)\n",
    "reviews['stop_comments'] = reviews['tokenized_comments'].apply(lambda x: [item for item in x if item not in stopset])\n",
    "def func(row):\n",
    "    return \" \".join(row)\n",
    "reviews['data'] = reviews['stop_comments'].apply(lambda x: func(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_topics(H,W,feature_names,documents,documents_id,listing_id,words,no_top_words,no_top_documents):\n",
    "    result_list = []\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        message = \" \".join(([feature_names[i]\n",
    "                             for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        top_doc_indices = np.argsort(W[:,topic_idx])[::-1][0:no_top_documents]\n",
    "        for doc_index in top_doc_indices:\n",
    "            result = (message,documents_id[doc_index],listing_id[doc_index],documents[doc_index],words[doc_index])\n",
    "            result_list.append(result)\n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_top_words = 3\n",
    "no_top_documents = 5\n",
    "result=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=reviews.groupby('listing_id').filter(lambda g: (g.listing_id.size >= 100))\n",
    "\n",
    "unique_listings = reviews.listing_id.unique()\n",
    "len(unique_listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each_listing in unique_listings:\n",
    "#     print(each_listing)\n",
    "    \n",
    "    listing_reviews=reviews[reviews['listing_id']==each_listing]\n",
    "    tf_vectorizer = TfidfVectorizer(max_df=0.85, min_df=2, max_features=10000)\n",
    "    tf = tf_vectorizer.fit_transform(listing_reviews['data'])\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    nmf_model = NMF(n_components= 3, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd')\n",
    "    nmf=nmf_model.fit(tf)\n",
    "    nmf_W = nmf.transform(tf)\n",
    "    nmf_H = nmf.components_\n",
    "    output = display_topics(nmf_H,nmf_W,tf_feature_names,list(listing_reviews['comments']),list(listing_reviews['id']),list(listing_reviews['listing_id']),list(listing_reviews['data']),no_top_words,no_top_documents)\n",
    "    result += output\n",
    "#     print(output)\n",
    "# np.save('nmf_H.npy', nmf_H)\n",
    "# np.save('nmf_tf.npy',tf)\n",
    "\n",
    "result_df = pd.DataFrame(result, columns=['Topic','Review_Id','Listing_Id','Documents','Words'])\n",
    "result_df.to_csv(\"Result_nmf_100.csv\",columns=[\"Topic\",\"Review_Id\",\"Listing_Id\",\"Documents\",\"Words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
